# ğŸ§  LSTM ile TÃ¼rkÃ§e Metin Ãœretimi

Bu proje, LSTM (Long Short-Term Memory) sinir aÄŸÄ± kullanÄ±larak TÃ¼rkÃ§e cÃ¼mleler Ã¼retmeyi amaÃ§lamaktadÄ±r. EÄŸitim verisi olarak GPT destekli yapay metinler kullanÄ±lmÄ±ÅŸ, modelin metin Ã¼retme yeteneÄŸi Ã§eÅŸitli tekniklerle geliÅŸtirilmiÅŸtir.

## âœ¨ Ã–zellikler

- ğŸ“š TÃ¼rkÃ§e dilinde kelime tabanlÄ± metin Ã¼retimi
- ğŸ”  N-gram dizileri ile sÄ±ralÄ± model eÄŸitimi
- ğŸ§Š Temperature (sÄ±caklÄ±k) ayarÄ±yla Ã§eÅŸitli metin Ã¼retimi
- ğŸ’¾ Model eÄŸitme, kaydetme ve yÃ¼kleme
- ğŸ” AnlamlÄ± ve dilbilgisel olarak dÃ¼zgÃ¼n cÃ¼mleler Ã¼retme

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

- Python 3.x
- TensorFlow / Keras
- NumPy
- JSON
- Regex (`re`)
- LSTM & Embedding katmanlarÄ±

## ğŸ“‚ Proje YapÄ±sÄ±
```
LSTM/
â”œâ”€â”€ metin_uretimi.py # Ana Python dosyasÄ±
â”œâ”€â”€ veriseti.json # EÄŸitim verisi (GPT ile oluÅŸturulmuÅŸ metinler)
â”œâ”€â”€ metin_uretimi.h5 # EÄŸitilmiÅŸ model dosyasÄ±
â””â”€â”€ tokenizer.pkl (isteÄŸe baÄŸlÄ±) # Tokenizer nesnesi (kullanÄ±lÄ±rsa)
```

## ğŸ§  Ã–ne Ã‡Ä±kan BaÅŸarÄ±mlar
Bu proje kapsamÄ±nda yalnÄ±zca metin Ã¼retimi yapÄ±lmamÄ±ÅŸ, aynÄ± zamanda aÅŸaÄŸÄ±daki Ã¶nemli noktalar baÅŸarÄ±yla gerÃ§ekleÅŸtirilmiÅŸtir:

âœ… **CÃ¼mle sonunu anlama yetisi:**  
Model, eÄŸitildiÄŸi verilerden Ã¶ÄŸrendiÄŸi dil yapÄ±sÄ± sayesinde noktalama iÅŸaretleri (., ?, !) ile cÃ¼mle sonunu anlamlÄ± yerlerde bitirmeyi Ã¶ÄŸrenmiÅŸtir.

âœ… **Temperature kullanÄ±mÄ± ile kontrollÃ¼ rastgelelik:**  
Ãœretilen metinlerde temperature (sÄ±caklÄ±k) parametresi kullanÄ±larak hem anlamlÄ±lÄ±k hem de Ã§eÅŸitlilik dengelenmiÅŸtir.  
â€¢ *DÃ¼ÅŸÃ¼k deÄŸerler* â†’ daha tutarlÄ±, tekrar eden yapÄ±lar  
â€¢ *YÃ¼ksek deÄŸerler* â†’ yaratÄ±cÄ± ama bazen anlamsÄ±z cÃ¼mleler  
â€¢ *Kademeli sÄ±caklÄ±k artÄ±ÅŸÄ±* â†’ daha doÄŸal cÃ¼mle uzamalarÄ±

âœ… **N-gram yapÄ±sÄ± ile baÄŸlamsal Ã¶ÄŸrenme:**  
EÄŸitim verisi, n-gram'lar ÅŸeklinde hazÄ±rlanarak kelimeler arasÄ± iliÅŸki Ã¶ÄŸrenilmiÅŸtir. BÃ¶ylece model tek tek kelimeleri deÄŸil, baÄŸlamÄ± Ã¶ÄŸrenmiÅŸtir.

âœ… **Noktalama duyarlÄ±lÄ±ÄŸÄ±:**  
JetonlayÄ±cÄ±da filtreleme yapÄ±lmadÄ±ÄŸÄ± iÃ§in noktalama iÅŸaretleri korunmuÅŸ, modelin dil bilgisine duyarlÄ± Ã¶ÄŸrenmesi saÄŸlanmÄ±ÅŸtÄ±r.


## ğŸ“ˆ Model BaÅŸarÄ±mÄ±

Modelin eÄŸitim sÃ¼recinde kaydedilen accuracy ve loss deÄŸerleri aÅŸaÄŸÄ±daki gibidir:
```
Epoch 1/100     - accuracy: 0.1194 - loss: 6.5205
Epoch 2/100     - accuracy: 0.1623 - loss: 5.6449
Epoch 3/100     - accuracy: 0.1664 - loss: 5.4198
...
Epoch 50/100    - accuracy: 0.7526 - loss: 1.7481
Epoch 52/100    - accuracy: 0.7868 - loss: 1.5922
...
Epoch 98/100    - accuracy: 0.9239 - loss: 0.3623
Epoch 100/100   - accuracy: 0.9209 - loss: 0.3560
```
ğŸ”¹ BaÅŸlangÄ±Ã§ta %12 doÄŸruluk ile baÅŸlayan model, 100 epoch sonunda %92 doÄŸruluk oranÄ±na ulaÅŸmÄ±ÅŸtÄ±r.  
ğŸ”¹ Her ne kadar bu, modelin baÅŸarÄ±lÄ± ÅŸekilde Ã¶ÄŸrendiÄŸini gÃ¶sterse de veri sayÄ±sÄ± az olduÄŸu iÃ§in aÅŸÄ±rÄ± Ã¶ÄŸrenme(overfitting) sorunu yaÅŸanmaktadÄ±r.

## ğŸ“ Ã–ÄŸrenilenler

- LSTM'nin sÄ±ralÄ± verilerdeki gÃ¼cÃ¼
- Tokenization ve n-gram oluÅŸturma
- Embedding katmanlarÄ± ile kelimeleri sayÄ±sal forma dÃ¶nÃ¼ÅŸtÃ¼rme
- Temperature ile Ã¼retimde rastgelelik kontrolÃ¼

## ğŸ“Œ Notlar

Bu proje temel bir Ã¶rnektir. Daha iyi sonuÃ§lar iÃ§in:
- Daha bÃ¼yÃ¼k ve Ã§eÅŸitli bir veri kÃ¼mesi kullanÄ±labilir.
- Bidirectional LSTM veya GRU gibi alternatif modeller denenebilir.
- TÃ¼rkÃ§e Ã¶zel karakterler dikkatlice iÅŸlenmelidir.
- Attention veya Transformer mimarisi ileride incelenebilir.
